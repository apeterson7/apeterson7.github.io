---
layout: post
title:  "Decoding Algorithms"
date:   2023-05-16 04:59:47 -0400
categories: nlp gpt bias
---

In [this notebook](https://colab.research.google.com/drive/1l2f7VZLVZjTdgP2Fk495wHdbu30drfzr?usp=sharing){:target="_blank"} experiment with BERT's performance on the masked language modeling objective.  I extend show the deterioration of linguistic acceptability when a span of tokens is masked as opposed to a single token.  I then implement greedy decoding, where the most probable token is selected at each time step, and ancestral decoding, where a token is selected from the entire probability distribution at each time step.  I then compare the continuations generated by each.